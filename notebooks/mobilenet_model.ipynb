{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18111e0a-653e-416e-bd93-9e6422bb2cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn import functional as F\n",
    "import torchmetrics\n",
    "import wandb\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05454c06-a807-494d-b3a1-7e2f7298fa33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpkantek\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20230103_005354-20ew4paf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/pkantek/f5611/runs/20ew4paf\" target=\"_blank\">lucky-sunset-2</a></strong> to <a href=\"https://wandb.ai/pkantek/f5611\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "wandb_logger = WandbLogger(project=\"f5611\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "703f0b09-f87a-40dd-a151-862a46f25b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir='../data', batch_size=64):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transform = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        CIFAR10(self.data_dir, train=True, download=True)\n",
    "        CIFAR10(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            cifar10 = CIFAR10(self.data_dir, train=True, transform=self.transform)\n",
    "            self.cifar_train, self.cifar_val = random_split(cifar10, [45000, 5000])\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.cifar_test = CIFAR10(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.cifar_train, batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.cifar_val, batch_size=10 * self.batch_size, num_workers=4)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.cifar_test, batch_size=10 * self.batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7eecce7-d430-47c1-9776-7cef7a8b7de6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_acc(n_classes):\n",
    "    if n_classes > 2:\n",
    "        return torchmetrics.Accuracy(task=\"multiclass\", num_classes=n_classes)\n",
    "    return torchmetrics.Accuracy(task=\"bianry\")\n",
    "\n",
    "# Define the PyTorch Lightning model\n",
    "class CIFAR10Model(pl.LightningModule):\n",
    "    def __init__(self, model, in_dims, n_classes=10, lr=1e-4):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "        self.save_hyperparameters(ignore=['model'])\n",
    "        \n",
    "        self.train_acc = get_acc(n_classes)\n",
    "        self.valid_acc = get_acc(n_classes)\n",
    "        self.test_acc = get_acc(n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits, loss = self.loss(x, y)\n",
    "        preds = torch.argmax(logits, 1)\n",
    "    \n",
    "        self.log('train/loss', loss, on_epoch=True)\n",
    "        self.train_acc(preds, y)\n",
    "        self.log('train/acc', self.train_acc, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits, loss = self.loss(x, y)\n",
    "        preds = torch.argmax(logits, 1)\n",
    "\n",
    "        self.valid_acc(preds, y)\n",
    "        self.log(\"valid/loss_epoch\", loss)  # default on val/test is on_epoch only\n",
    "        self.log('valid/acc_epoch', self.valid_acc)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        dummy_input = torch.zeros(self.hparams[\"in_dims\"], device=self.device)\n",
    "        model_filename = f\"model_{str(self.global_step).zfill(5)}.onnx\"\n",
    "        torch.onnx.export(self, dummy_input, model_filename, opset_version=11)\n",
    "        artifact = wandb.Artifact(name=\"model.ckpt\", type=\"model\")\n",
    "        artifact.add_file(model_filename)\n",
    "        self.logger.experiment.log_artifact(artifact)\n",
    "\n",
    "        flattened_logits = torch.flatten(torch.cat(validation_step_outputs))\n",
    "        self.logger.experiment.log(\n",
    "            {\"valid/logits\": wandb.Histogram(flattened_logits.to(\"cpu\")),\n",
    "             \"global_step\": self.global_step})\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits, loss = self.loss(x, y)\n",
    "        preds = torch.argmax(logits, 1)\n",
    "\n",
    "        self.test_acc(preds, y)\n",
    "        self.log(\"test/loss_epoch\", loss, on_step=False, on_epoch=True)\n",
    "        self.log(\"test/acc_epoch\", self.test_acc, on_step=False, on_epoch=True)\n",
    "    \n",
    "    def test_epoch_end(self, test_step_outputs):  # args are defined as part of pl API\n",
    "        dummy_input = torch.zeros(self.hparams[\"in_dims\"], device=self.device)\n",
    "        model_filename = \"model_final.onnx\"\n",
    "        self.to_onnx(model_filename, dummy_input, export_params=True)\n",
    "        artifact = wandb.Artifact(name=\"model.ckpt\", type=\"model\")\n",
    "        artifact.add_file(model_filename)\n",
    "        wandb.log_artifact(artifact)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits, loss = self.loss(x, y)\n",
    "        preds = torch.argmax(logits, 1)\n",
    "\n",
    "        self.valid_acc(preds, y)\n",
    "        self.log(\"valid/loss_epoch\", loss)  # default on val/test is on_epoch only\n",
    "        self.log('valid/acc_epoch', self.valid_acc)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        dummy_input = torch.zeros(self.hparams[\"in_dims\"], device=self.device)\n",
    "        model_filename = f\"model_{str(self.global_step).zfill(5)}.onnx\"\n",
    "        torch.onnx.export(self, dummy_input, model_filename, opset_version=11)\n",
    "        artifact = wandb.Artifact(name=\"model.ckpt\", type=\"model\")\n",
    "        artifact.add_file(model_filename)\n",
    "        self.logger.experiment.log_artifact(artifact)\n",
    "\n",
    "        flattened_logits = torch.flatten(torch.cat(validation_step_outputs))\n",
    "        self.logger.experiment.log(\n",
    "            {\"valid/logits\": wandb.Histogram(flattened_logits.to(\"cpu\")),\n",
    "             \"global_step\": self.global_step})\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams[\"lr\"])\n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        logits = self(x)\n",
    "        cel = torch.nn.CrossEntropyLoss()\n",
    "        loss = cel(logits, y)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13be4cf0-6f31-42a0-839c-7c4515ee9cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePredictionLogger(pl.Callback):\n",
    "    def __init__(self, val_samples, num_samples=32):\n",
    "        super().__init__()\n",
    "        self.val_imgs, self.val_labels = val_samples\n",
    "        self.val_imgs = self.val_imgs[:num_samples]\n",
    "        self.val_labels = self.val_labels[:num_samples]\n",
    "          \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        val_imgs = self.val_imgs.to(device=pl_module.device)\n",
    "\n",
    "        logits = pl_module(val_imgs)\n",
    "        preds = torch.argmax(logits, 1)\n",
    "\n",
    "        trainer.logger.experiment.log({\n",
    "            \"examples\": [wandb.Image(x, caption=self.labels_to_caption(pred, y)) \n",
    "                            for x, pred, y in zip(val_imgs, preds, self.val_labels)],\n",
    "            \"global_step\": trainer.global_step\n",
    "            })\n",
    "        \n",
    "    def labels_to_caption(self, pred, y):\n",
    "        mapping = {0: \"airplane\", 1:\"car\", 2:\"bird\", 3:\"cat\", 4:\"deer\", 5:\"dog\", 6:\"frog\", 7:\"horse\", 8:\"ship\", 9: \"truck\"}\n",
    "        pred_name = mapping[pred.item()]\n",
    "        y_name = mapping[y.item()]\n",
    "        return f\"Pred:{pred_name}, Label:{y_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68b577ff-a848-4121-9853-4808dfb996a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet = torchvision.models.mobilenet_v3_large(weights=torchvision.models.MobileNet_V3_Large_Weights.IMAGENET1K_V2, progress=True)\n",
    "mobilenet.classifier[3] = nn.Linear(in_features=1280, out_features=10, bias=True)\n",
    "mobilenet.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c19ace1-715f-4c1e-aea1-349555809e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# setup data\n",
    "cifar = CIFAR10DataModule()\n",
    "cifar.prepare_data()\n",
    "cifar.setup()\n",
    "\n",
    "# grab samples to log predictions on\n",
    "samples = next(iter(cifar.val_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dc35a63-0e6e-4963-aeb3-b881687dd870",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    logger=wandb_logger,    # W&B integration\n",
    "    log_every_n_steps=50,   # set the logging frequency\n",
    "    max_epochs=5,           # number of epochs\n",
    "    deterministic=True,     # keep it deterministic\n",
    "    callbacks=[ImagePredictionLogger(samples)] # see Callbacks section\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c95c81fa-6953-4d6a-985f-fc9220072c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkantek\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ml-astr-wIJ34D6o-py3.10\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:262: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | model     | MobileNetV3        | 4.2 M \n",
      "1 | train_acc | MulticlassAccuracy | 0     \n",
      "2 | valid_acc | MulticlassAccuracy | 0     \n",
      "3 | test_acc  | MulticlassAccuracy | 0     \n",
      "-------------------------------------------------\n",
      "4.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 M     Total params\n",
      "16.859    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 32, 32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6735bc11d374c0a94982c070ab859b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 32, 32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 32, 32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 32, 32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 32, 32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 32, 32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n",
      "C:\\Users\\pkantek\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ml-astr-wIJ34D6o-py3.10\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\checkpoint_connector.py:134: UserWarning: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at .\\f5611\\20ew4paf\\checkpoints\\epoch=4-step=3520.ckpt\n",
      "Loaded model weights from checkpoint at .\\f5611\\20ew4paf\\checkpoints\\epoch=4-step=3520.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51812b2d1c384408923417de8cfef213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test/acc_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6718000173568726     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test/loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1043251752853394     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test/acc_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6718000173568726    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test/loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1043251752853394    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▃▃▃▃▃▃▃▃▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆████████</td></tr><tr><td>global_step</td><td>▁▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>test/acc_epoch</td><td>▁</td></tr><tr><td>test/loss_epoch</td><td>▁</td></tr><tr><td>train/acc_epoch</td><td>▁▄▆▇█</td></tr><tr><td>train/acc_step</td><td>▁▁▂▄▄▅▄▄▄▅▅▆▆▆▅▅▆▆▆▆▆▆▆▆▇▅▇▇▇██▆▇▇██▇███</td></tr><tr><td>train/loss_epoch</td><td>█▅▃▂▁</td></tr><tr><td>train/loss_step</td><td>█▇▇▅▅▄▅▄▄▄▄▃▄▃▄▄▃▂▃▂▃▃▂▃▂▃▂▂▂▁▁▂▂▂▁▁▂▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>valid/acc_epoch</td><td>▁▆▇██</td></tr><tr><td>valid/loss_epoch</td><td>█▃▁▂▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>global_step</td><td>3520</td></tr><tr><td>test/acc_epoch</td><td>0.6718</td></tr><tr><td>test/loss_epoch</td><td>1.10433</td></tr><tr><td>train/acc_epoch</td><td>0.83258</td></tr><tr><td>train/acc_step</td><td>0.82812</td></tr><tr><td>train/loss_epoch</td><td>0.48509</td></tr><tr><td>train/loss_step</td><td>0.43491</td></tr><tr><td>trainer/global_step</td><td>3520</td></tr><tr><td>valid/acc_epoch</td><td>0.6588</td></tr><tr><td>valid/loss_epoch</td><td>1.18417</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">lucky-sunset-2</strong>: <a href=\"https://wandb.ai/pkantek/f5611/runs/20ew4paf\" target=\"_blank\">https://wandb.ai/pkantek/f5611/runs/20ew4paf</a><br/>Synced 5 W&B file(s), 192 media file(s), 7 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230103_005354-20ew4paf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# setup model\n",
    "model = CIFAR10Model(model=mobilenet, in_dims=(1, 3, 32, 32))\n",
    "\n",
    "# fit the model\n",
    "trainer.fit(model, cifar)\n",
    "\n",
    "# evaluate the model on a test set\n",
    "trainer.test(datamodule=cifar,\n",
    "             ckpt_path=None)  # uses last-saved model\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4516cf9d-6391-4c79-bcdd-fe5859ec9a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a8ac86-9ca7-4fa8-b285-9d50c8013910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

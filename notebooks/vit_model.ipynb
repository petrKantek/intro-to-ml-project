{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc4585b-e8f9-4583-9b18-2d2cfbe500b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CIFAR-10 dataset\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((384, 384)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Define the model\n",
    "model = torchvision.models.VisionTransformer(\n",
    "    image_size=32, patch_size=8, num_layers=6, num_heads=8, hidden_dim=512, mlp_dim=10\n",
    ")\n",
    "\n",
    "model = torchvision.models.vit_b_16(weights=torchvision.models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1)\n",
    "\n",
    "\n",
    "# Replace the model's final linear layer\n",
    "model.classifier = nn.Linear(in_features=2048, out_features=10)\n",
    "\n",
    "\n",
    "# # Load the pretrained weights\n",
    "# model.load_state_dict(torch.load('pretrained_weights.pth'))\n",
    "\n",
    "# Freeze the parameters of the pretrained model\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "    \n",
    "# # Unfreeze the final layer and set it to train mode\n",
    "# for param in model.classifier.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "model.train()\n",
    "\n",
    "# for idx, tensor in enumerate(model_mob.parameters()):\n",
    "#     if idx < 173:\n",
    "#         tensor.requires_grad = False\n",
    "\n",
    "for tensor in model_mob.parameters():\n",
    "    tensor.requires_grad = False\n",
    "\n",
    "for tensor in model_mob.classifier.parameters():\n",
    "    tensor.requires_grad = True\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize the DataModule and Model\n",
    "data_module = CIFAR10DataModule(train_dataset, val_dataset)\n",
    "lightning_model = CIFAR10Model(model_mob, loss_fn, optimizer)\n",
    "\n",
    "\n",
    "from pytorch_lightning.callbacks import RichProgressBar\n",
    "\n",
    "# Define the trainer\n",
    "# trainer = pl.Trainer(max_epochs=10, default_root_dir='../logs', logger=wandb_logger, callbacks=RichProgressBar(leave=True))\n",
    "trainer = pl.Trainer(max_epochs=10, default_root_dir='../logs', logger=wandb_logger)\n",
    "\n",
    "\n",
    "# Start training\n",
    "trainer.fit(lightning_model, data_module)\n",
    "\n",
    "\n",
    "# Define the PyTorch Lightning DataModule\n",
    "# class CIFAR10DataModule(pl.LightningDataModule):\n",
    "#     def __init__(self, train_dataset, val_dataset):\n",
    "#         super().__init__()\n",
    "#         self.train_dataset = train_dataset\n",
    "#         self.val_dataset = val_dataset\n",
    "\n",
    "#     # def setup(self, stage=None):\n",
    "#     #     self.train_dataloader = torch.utils.data.DataLoader(self.train_dataset, batch_size=64, shuffle=True)\n",
    "#     #     self.val_dataloader = torch.utils.data.DataLoader(self.val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "#     def train_dataloader(self):\n",
    "#         return torch.utils.data.DataLoader(self.train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "\n",
    "#     def val_dataloader(self):\n",
    "#         return torch.utils.data.DataLoader(self.val_dataset, batch_size=128, shuffle=False, num_workers=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
